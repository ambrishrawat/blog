---
layout: post
title:  "Create to understand"
date:   2016-12-08 22:56:35 +0000
categories: 
comments: false
use_math: true
---

## Where does the story begin?

Generative modelling is one of the most fascinating paradigms of AI. And the story of generative modelling begins at observations of phenomena. Colloquially, we associate phenomenon with an object of discussion, when we believe that the object somehow came into being, that it exists because it got created, or somehow generated. Often, in order to understand such a phenomemnon, we attempt to recreate or mimic its underlying generative process. One such phenomemon that AI has been successfull at mimicing is natural speech. AI claims to 'understand' spoken speech through a surgical breakdown of how elemental forms like sound waves, phonemes, words and sentences are strung together in its synthesis.

### A classical generator

Often my kneejerk reaction to a large sequence of real numbers is computing their mean and variance. Although mathemtically grounded in law of large numbers, implicit to this computation is a notion of existence. I believe that there exists a 'true' distribution from which the numbers were generated. In fact, by extrapolating or matching the sample moments to the moments of my believed distribution, I can write a generator that samples numbers to the ones observed. For instance, , in Python. These random-samplers used in various programming languages are a classical examples of generative models. With chained conditioning, these basic generators can be further combined to represent other complex distributions. This marriage of graph theory and statistics has developed as a field in itself as Probabilistic Graphical Models.

Let's say we were to write generative process that could have led to N observations, $$\{x_i\}^N_1$$. By hook or crook, or some mental-gymnastics one might be able to engineer a generative process for low dimensiona $$x$$. However, things can be complex as distrubtions become highly multi-modal . Moreover, there is also a challenging task is in verifying if the generated samples even make sense. 
One reliable way to validate the sample-generation process, is by estimating the density of generated samples. But there are situations when this is infeasible. There are two interesting approaches to when the generative process is intricate and  which I wish to look a closer look at.

### The a b c of simulation models

Complex phenomenon like weather, brain, genetics or the Big-Bang have been studied by humans for centuries. Their mysteriousness rarely fails to strike a chord with our curiosity. While studying these phenomenon, we often simulate their involved dynamics through large complex computer-programs. 


### Generating by merely observing

Phenomemon of natural images. 

#### Generative Adversarial Networks

predicting without observing

#### Variational Autoencoders


