---
layout: post
title:  "An introduction to Introduction to Artificial Intelligence"
date:   2016-11-25 22:56:35 +0000
categories: 
comments: true
---

Textbooks offer a reliable, detailed and formal introduction to most subjects. However, introducing Machine Learning and AI isn’t the easiest nut to crack. There are disagreements within the community about what does and what doesn’t qualify as ML or AI. And then there are different philosophical viewpoints. Consider the modern wonder of Deep Learning, for instance, where different schools of thoughts (from cybernetics to connectionism to deep neural networks) have waxed and waned in popularity throughout the course of its history. And then there are the fundamentals of Probability theory whose formulation, both diverse and contentious, has evolved over centuries of thought. So I will begin with a less ambitious goal of sketching an introduction to introduction to ML. I will draw some motivation from differences in Mathematics and Physics textbooks with a hope to shed light on some qualitative aspects of ML and AI. 


During my undergraduate education I managed to get hooked on to a few textbooks, two of which were  - Introduction to Topology and Modern Analysis by G. F. Simmons, and Feynman Lectures in Physics. Like most traditional texts in mathematics, Simmons’ book had a fairly regular structure. Each chapter had an introduction, followed by some to-the-point definitions which quickly led to the meat of the content  - theorems, proofs, lemmas and conjectures. The first two sections were tersely written. With the minimal usage of words, their rereads would often be carried out as news bulletins rituals. Simmons, however, was less conservative in the latter sections where the text had ample explanation and examples, making it a more natural read. Definitions and theorems were distinctly marked out with a font formatting. As if borrowed from a bag of universal truths, they were untouched by the language of the text. 


Introductions in Feynman’s book or more generally in physics textbooks were vastly different. With the marvels and wonders the chapters promised of demystifying, I would often read, re-read and re-re-read the introductions itself. The scale of mysteries spanned across all powers of ten, from macroscopic galaxies to microscopic atoms and the accompaying prose was absolutely fascinating. In the text that followed, the trait of indisputability and universality lied with observations or phenomenon and not with the laws or theory. A theory was susceptible to change. It evolved as it was extended, challenged questioned, or debunked unlike the mathematical theorems where the truths were merely appended or reinterpretted. 


Machine Learning, or the broader Artificial Intelligence emerged out as fields, or more strictly as terms, relatively recently in the taxonomy of science. Moreover, as they borrowed principles from a variety of other domains, there were multiple entry points available for introduction. For instance, an introductory texts could captivate a reader’s attention through aspects of human cognition or the meaning of intelligence. Equivalently, the text could delve into the conceptual frameworks for supervised-unsupervised learning, representation learning, graphical models or the 101 on statistics of regression. All of these vantage points are convincing as introductions but there are rarely beaded with definitions carved in stone. Contrary to my desire or taste, the defintions are fluidic, and pertinent only under context. Ironically the worst off is the elemental unit they all share and talk about, a model, which is unmistakably the most loosely defined term in ML’s vocabulary. Its amorphic quality is best examined through George EP Box’ epiphanic remark - *All models are wrong but some are useful.*

## Where does the story begin?

It’s not all that bad.

The universality of mathematical reasoning can in many cases be reduced to logic, and most commonly, aristotalian logic, where a proposition is either true or false. As the propositions come together in a proof, the underlying reasoning surfaces out. This ascribes a character of clarity to the reasoned arguments which is one the hallmarks of this reasoning framework. The theory and hypothesis of physics is similarly backed with observations and empirical measurements of physical phenomenon. Classical thermodynamics is a quintessential example of this which as Einstein famously quoted, “*is the only physical theory of universal content which I am convinced will never be overthrown.*” 

In similar fashion, ML is borne out of reasoning under approximate knowledge. This setting closely mimics human reasoning or common sense, where things are rarely black and white, and arguments and conclusions are made under assumptions. This correspondence to common sense has been studied and demonstrated by many writers (Keynes, Jeffery, Polya, Cox, Tribus, de Fenneti and Rosenkrantz). And as with physical processes, data or observations substantiate as strongest evidence, which provides constant encouragement to a practitioner who is invested in the quest for better explanations. 

The seemingly restrictive connotation of multiple viewpoints and vague definitions is surprisingly a strong suite of ML. ML offers a unique environment that embraces and encourages different approaches. It’s like wearing different hats in a detective investigation, each bestowed with its own set of rules and tradeoffs. Why inspect it from only one angle, when you could do it from two? Or even more? The jargon of ML is vast, and with the current buzz-ness surrounding ML and AI, the vocabulary is only getting larger, and boundaries are getting thinner. So perhaps it is sensible to shed away from a strict definition for ML and make peace with the expression, you’ll know it when you see it. In addition, an ML way of thinking needs to be motivated. Why a particular modeling approach? What's the broader context? Where do things unify? In short, what's the story and where does it begin?
